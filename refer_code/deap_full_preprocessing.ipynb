{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3615e04d",
   "metadata": {},
   "source": [
    "# DEAP Dataset Preprocessing\n",
    "\n",
    "This Notebook can be used in order to preprocess the DEAP Dataset in the following way: <br/>\n",
    "1. Collect all the Data form the mat files\n",
    "2. Remove the baseline from the SEEG Signals\n",
    "3. Filter the EEG Signals, either with a bandpass filter, a lowpass filter or a highpass filter\n",
    "4. Downsample the EEG Signals to a certain sampling rate\n",
    "5. Select only certain EEG Channels and rearange them in a given order\n",
    "6. Cut the EEG Signals into Windows of given length with given overlap\n",
    "7. Convert the Labels from Valence, Arousal, Dominance into the Classes Negative, Neutral, Positive\n",
    "8. Safe the generated Dataset\n",
    "9. Generate some decent plots to compare the old signal with the new signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be93100",
   "metadata": {},
   "source": [
    "## Description, Dataset Access and Citation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ecfbe5",
   "metadata": {},
   "source": [
    "<b>For a full description and access to the Dataset please see:</b> <br/>\n",
    "url: http://www.eecs.qmul.ac.uk/mmv/datasets/deap/readme.html <br/>\n",
    "[1] Koelstra, S.; Muhl, C.; Soleymani, M.; Lee, J.-S.; Yazdani, A.; Ebrahimi, T.; Pun, T.;Nijholt, A.; Patras, I.: Deap: A Database for Emotion Analysis using Physiological Signals. IEEE transactions on affective computing 3 (2011) 1, p. 18–31.<br/>\n",
    "<b>If you find this code helpfull please cite:</b><br/>\n",
    "tbd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba7a022",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4afaead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir ='../Datasets/raw/deap/data_preprocessed_python'\n",
    "# output_dir = '../Datasets/full_preprocessing/'\n",
    "data_dir ='E:/Databases/DEAP/data_preprocessed_python/'\n",
    "output_dir = 'E:/Databases/DataPre/DEAP/full_preprocessing/'\n",
    "baseline_removal_window = 3\n",
    "cutoff_frequencies = [4,40]\n",
    "seconds_to_use = 60\n",
    "downsampling_rate = 128\n",
    "channels_to_use = ['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4']\n",
    "window_size = 2\n",
    "window_overlap = 0\n",
    "convert_labels_to_nnp = True\n",
    "save_plots_to_file = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81ae0381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(os.path.join(output_dir, \"figures\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4dcb8f",
   "metadata": {},
   "source": [
    "<b>data dir (String)</b>               - the path to the raw Dataset, e.g. \"'../Datasets/raw/deap/data_preprocessed_python'\" <br/>\n",
    "<b>output dir (String)</b>             - the path, where you want to save the preprocessed Dataset, e.g.                                                   \"myDatasets/DEAP/ <br/>\n",
    "<b>baseline_removal_window (float)</b> - the timewindow, to calculate a baseline (average) from. This value gets                                             subtracted from the whole timeseries. If you don't want to use baseline_removal,                                   set it to 0 <br/>\n",
    "<b>cutoff_frequencies (touple)</b>     - the cutoff frequencies for the filter, if the first value ist set to None,                                         allowpass-filtering is used, if the second value is set to None, a highpass filt                                   ist used, if both values are set, a bandpass filter is used <br/>\n",
    "<b>seconds_to_use (int)</b> - The window in seconds to use from the timeseries, if for example seconds_to_use ist set to 45, only the last 45 seconds will be used. If you want to use the whole timeseries, set it to None <br/>\n",
    "<b>downsampling_rate (int)</b>         - the frequency, to which the eeg signals should be downsampled. If you don't want to downsample the signal, set it to 0 <br/>\n",
    "<b>channels_to_use (list)</b> a list of eeg channels you want to use, if you want to use all channels from the dataset, set it to None<br/>\n",
    "<b>window_size (int)</b>              - the lenght of the timewindow in seconds, the dataset should be cut into <br/>\n",
    "<b>window_overlap (int)</b>            - overlap of the windows in seconds, if set to 0, the windows don't overlap <br/>\n",
    "<b>convert_labels_to_nnp (boolean)</b> - wheter or not you want to convert the labels of the dataset from valence, arousal, liking, dominance to negative, neutral, positive <br/>\n",
    "<b>save_plots_to_file (boolean)</b> - wheter or not you want to save the generated plots, if you choose False, the plots will be only shown in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851cbb79",
   "metadata": {},
   "source": [
    "## necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17dedaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import os\n",
    "import warnings\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526af223",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e8c8d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 128\n",
    "channel_names = ['FP1', 'AF3', 'F3', 'F7', 'FC5', 'FC1', 'C3', 'T7', 'CP5', 'CP1', 'P3', 'P7', 'PO3', 'O1', 'OZ', 'PZ', 'FP2', 'AF4', 'FZ', 'F4', 'F8', 'FC6', 'FC2', 'CZ', 'C4', 'T8', 'CP6', 'CP2', 'P4', 'P8', 'PO4', 'O2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab822262",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.empty((1280,32,8064))\n",
    "X_len = np.empty((1280,), dtype=int)\n",
    "Y = np.empty((1280,4))\n",
    "trial = np.empty((1280), dtype=np.int8)\n",
    "subject = np.empty((1280,), dtype=np.int8)\n",
    "session = np.ones((1280,), dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5882b57e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e2b1ac81334f75a1a5e342dd87d034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "file_id = 0\n",
    "for index, filename in tqdm(enumerate(sorted(os.listdir(data_dir)),start=1)):\n",
    "    if filename.endswith(\".dat\"):\n",
    "        temp_file = pickle.load(open(os.path.join(data_dir, filename), 'rb'), encoding='iso-8859-1')\n",
    "        X_len[file_id*40:file_id*40+40] = np.ones((40,))*temp_file['data'].shape[2]\n",
    "        X[file_id*40:file_id*40+40,:,:] = temp_file['data'][:,:32,:]\n",
    "        Y[file_id*40:file_id*40+40,:] = temp_file['labels']\n",
    "        subject[file_id*40:file_id*40+40] = np.ones((40,), dtype=np.int8)* (file_id+1)\n",
    "        trial[file_id*40:file_id*40+40] = np.arange(1,41)\n",
    "        file_id = file_id +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c08e64c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Time Series Array X: (1280, 32, 8064)\n",
      "Shape of the Time Series Array Y: (1280, 4)\n",
      "Unique Session Indices: [1]\n",
      "Unique Subject Indices: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32]\n",
      "Unique Trial Indices: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40]\n",
      "Minimum length of Timeseries: 8064\n",
      "Maximum length of Timeseries: 8064\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the Time Series Array X: \" + str(X.shape))\n",
    "print(\"Shape of the Time Series Array Y: \" + str(Y.shape))\n",
    "print(\"Unique Session Indices: \" + str(np.unique(session)))\n",
    "print(\"Unique Subject Indices: \" + str(np.unique(subject)))\n",
    "print(\"Unique Trial Indices: \" + str(np.unique(trial)))\n",
    "print(\"Minimum length of Timeseries: \" + str(min(X_len)))\n",
    "print(\"Maximum length of Timeseries: \" + str(max(X_len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "702d3059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_raw is later used for plotting, if you don't want to see the plots, you can uncomment this line\n",
    "X_raw = X.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1c39ea",
   "metadata": {},
   "source": [
    "## Baseline-Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dd53f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681fa4d055ba4071a506fa0d5ad3531c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8064 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not(baseline_removal_window==0):\n",
    "    baseline_datapoints = baseline_removal_window * sampling_rate\n",
    "    baseline = X[:,:,:baseline_datapoints].sum(2) / baseline_datapoints\n",
    "    for timestep in trange(X.shape[2]):\n",
    "        X[:,:,timestep] = X[:,:,timestep] - baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014418e4",
   "metadata": {},
   "source": [
    "## Filtering (Bandpass or Highpass or Lowpass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b262a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import butter, sosfilt, sosfreqz\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, btype='band', order=5):\n",
    "        nyq = 0.5 * fs\n",
    "        if btype == 'bandpass':\n",
    "            low = lowcut / nyq\n",
    "            high = highcut / nyq\n",
    "            sos = butter(order, [low, high], analog=False, btype='bandpass', output='sos')\n",
    "        elif btype == 'highpass':\n",
    "            low = lowcut / nyq\n",
    "            sos = butter(order, low, analog=False, btype='highpass', output='sos')\n",
    "        elif btype == 'lowpass':\n",
    "            high = highcut / nyq\n",
    "            sos = butter(order, high, analog=False, btype='lowpass', output='sos')\n",
    "        return sos\n",
    "\n",
    "def butter_bandpass_filter(X, lowcut, highcut, fs, btype='bandpass', order=5):\n",
    "        sos = butter_bandpass(lowcut, highcut, fs, btype=btype, order=order)\n",
    "        X = sosfilt(sos, X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab52cfca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61916fdb73b44d79b5905d95207018c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1280 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not(cutoff_frequencies[0] == None):\n",
    "    if not(cutoff_frequencies[1] == None):\n",
    "        btype='bandpass'\n",
    "    else:\n",
    "        btype='highpass'\n",
    "elif not (cutoff_frequencies[1] == None):\n",
    "        btype='lowpass'\n",
    "\n",
    "for experiment_id in trange(X.shape[0]):\n",
    "    for channel_id in range(X.shape[1]):\n",
    "        X[experiment_id, channel_id, :] = butter_bandpass_filter(\n",
    "                                                        X[experiment_id, channel_id, :],\n",
    "                                                        cutoff_frequencies[0],\n",
    "                                                        cutoff_frequencies[1],\n",
    "                                                        sampling_rate,\n",
    "                                                        btype=btype,\n",
    "                                                        order=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "814f4106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (1280, 32, 8064)\n"
     ]
    }
   ],
   "source": [
    "print(f\"x: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b8e642",
   "metadata": {},
   "source": [
    "## Use only the last n seconds of the timeseries\n",
    "(As determined with the hyperparameter seconds_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ee1f399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7f284ed72f8430dab1ed727fb2556c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1280 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not(seconds_to_use == None):\n",
    "    num_sample_points_to_use = seconds_to_use * sampling_rate\n",
    "    X_selected = np.zeros((X.shape[0], X.shape[1], num_sample_points_to_use))\n",
    "    for exp_id in trange(len(X_len)):\n",
    "        X_selected[exp_id,:,:] = X[exp_id,:,X_len[exp_id]-num_sample_points_to_use:X_len[exp_id]]\n",
    "    X = X_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "792c2629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (1280, 32, 7680)\n"
     ]
    }
   ],
   "source": [
    "print(f\"x: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee886ac4",
   "metadata": {},
   "source": [
    "## Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22f7d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4681edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(downsampling_rate == 0) and not(downsampling_rate == sampling_rate):\n",
    "    new_length = int(X.shape[2] / sampling_rate * downsampling_rate)\n",
    "    X_downsampled = np.zeros((X.shape[0], X.shape[1], new_length))\n",
    "    for experiment_id in trange(X.shape[0]):\n",
    "        for channel_id in range(X.shape[1]):\n",
    "            X_downsampled[experiment_id, channel_id, :] = resample(X[experiment_id, channel_id, :], new_length)\n",
    "    X = X_downsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e366fa5",
   "metadata": {},
   "source": [
    "## Select certain channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56c0c6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4']\n"
     ]
    }
   ],
   "source": [
    "if channels_to_use == None:\n",
    "    channels_to_use = channel_names\n",
    "print(channels_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "174e654f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 2, 4, 7, 11, 13, 31, 29, 25, 21, 19, 20, 17]\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "channel_index_list = list()\n",
    "for i in range(len(channels_to_use)):\n",
    "    if channels_to_use[i] in channel_names:\n",
    "        channel_index_list.append(channel_names.index(channels_to_use[i]))\n",
    "    else:\n",
    "        warnings.warn(' Channel ' + channels_to_use[i] +' could not be found in the list of actual channels')\n",
    "print(channel_index_list)\n",
    "print(len(channel_index_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05ffc734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6366d7be72794dc8a6da4deb77bf903d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_selected_channels = np.zeros((X.shape[0], len(channels_to_use), X.shape[2]))\n",
    "for channel in trange(len(channel_index_list)):\n",
    "    X_selected_channels[:,channel,:] = X[:,channel_index_list[channel],:]\n",
    "X = X_selected_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ced9eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (1280, 14, 7680)\n"
     ]
    }
   ],
   "source": [
    "print(f\"x: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80727c76",
   "metadata": {},
   "source": [
    "## Cut into windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8466137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1638a1a722b4deba4bac8c4db96c9fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1280 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 计算每个窗口的点数和每个窗口的重叠点数\n",
    "num_points_per_window = window_size * downsampling_rate\n",
    "num_points_overlap = window_overlap * downsampling_rate\n",
    "\n",
    "# 计算窗口滑动的步长\n",
    "stride = num_points_per_window - num_points_overlap\n",
    "\n",
    "# 初始化起始和结束索引列表\n",
    "start_index = [0]\n",
    "end_index = [num_points_per_window]\n",
    "\n",
    "# 初始化每个实验的窗口数\n",
    "num_windows_per_exp = 1\n",
    "\n",
    "# 计算每个实验的窗口数以及相应的起始和结束索引\n",
    "while(end_index[-1] + stride < X.shape[2]):\n",
    "    num_windows_per_exp += 1\n",
    "    start_index.append(start_index[-1] + stride)\n",
    "    end_index.append(end_index[-1] + stride)\n",
    "\n",
    "# 初始化切割后的数据数组\n",
    "X_cut = np.zeros((num_windows_per_exp * X.shape[0], X.shape[1], num_points_per_window))\n",
    "Y_cut = np.zeros((num_windows_per_exp * X.shape[0], 4))\n",
    "session_cut = np.zeros(num_windows_per_exp * X.shape[0])\n",
    "subject_cut = np.zeros(num_windows_per_exp * X.shape[0])\n",
    "trial_cut = np.zeros(num_windows_per_exp * X.shape[0])\n",
    "\n",
    "# 遍历每个实验\n",
    "for exp_id in trange(X.shape[0]):\n",
    "    # 遍历每个窗口\n",
    "    for window_id in range(len(start_index)):\n",
    "        # 根据窗口的起始和结束索引切割数据\n",
    "        X_cut[exp_id * num_windows_per_exp + window_id, :, :] = X[exp_id, :, start_index[window_id]:end_index[window_id]]\n",
    "        # 复制标签、会话、受试者和试验信息\n",
    "        Y_cut[exp_id * num_windows_per_exp + window_id, :] = Y[exp_id, :]\n",
    "        session_cut[exp_id * num_windows_per_exp + window_id] = session[exp_id]\n",
    "        subject_cut[exp_id * num_windows_per_exp + window_id] = subject[exp_id]\n",
    "        trial_cut[exp_id * num_windows_per_exp + window_id] = trial[exp_id]\n",
    "\n",
    "# 更新原始数据为切割后的数据\n",
    "X = X_cut\n",
    "Y = Y_cut\n",
    "session = session_cut\n",
    "subject = subject_cut\n",
    "trial = trial_cut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "689826ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37120, 14, 256)\n",
      "session shape: (37120,)\n",
      "session: [1.]\n",
      "subject: [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\n",
      " 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32.]\n",
      "trial: [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\n",
      " 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36.\n",
      " 37. 38. 39. 40.]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(f\"session shape: {np.array(session).shape}\")\n",
    "print(f\"session: {np.unique(session)}\")\n",
    "print(f\"subject: {np.unique(subject)}\")\n",
    "print(f\"trial: {np.unique(trial)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cb3095",
   "metadata": {},
   "source": [
    "## Label Conversion\n",
    "The Labels get converted from Valence Arousal Dominance Liking into the Classes Negative, Neutral, Positive using a <b>K-Means Clustering </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f7a8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65739882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_centeroids(centeroids):\n",
    "    # calculates the distance between given centeroids\n",
    "    # and the corners bottom-right, bottom-left, top-left, top-right\n",
    "    # and returns them in the order that the point closest to the br corner is returned first,\n",
    "    # the one closest to the bl second,\n",
    "    # the one clostest to the tl third,\n",
    "    # and the one closest to the top right fourth\n",
    "    \n",
    "    distance_br = np.zeros(centeroids.shape[0])\n",
    "    distance_bl = np.zeros(centeroids.shape[0])\n",
    "    distance_tl = np.zeros(centeroids.shape[0])\n",
    "    distance_tr = np.zeros(centeroids.shape[0])\n",
    "    \n",
    "    for i in range(centeroids.shape[0]):\n",
    "        distance_br[i] = abs((1-centeroids[i,0])**2 + (-1-centeroids[i,1])**2)\n",
    "        distance_bl[i] = abs((-1-centeroids[i,0])**2 + (-1-centeroids[i,1])**2)\n",
    "        distance_tl[i] = abs((-1-centeroids[i,0])**2 + (1-centeroids[i,1])**2)\n",
    "        distance_tr[i] = abs((1-centeroids[i,0])**2 + (1-centeroids[i,1])**2)\n",
    "    \n",
    "    br_idx = np.argmin(distance_br)\n",
    "    bl_idx = np.argmin(distance_bl)\n",
    "    tl_idx = np.argmin(distance_tl)\n",
    "    tr_idx = np.argmin(distance_tr)\n",
    "    \n",
    "    return br_idx, bl_idx, tl_idx, tr_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972652aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "valence = scaler.fit_transform(Y[:,0].reshape(-1,1))\n",
    "arousal = scaler.fit_transform(Y[:,1].reshape(-1,1))\n",
    "datapoints = np.concatenate((valence,arousal),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a0bbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=7).fit(datapoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa06435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sad_label, fear_label, neutral_label, happy_label = sort_centeroids(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5781b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(valence,arousal, c=kmeans.labels_, edgecolors='none')\n",
    "plt.xlabel('Valence')\n",
    "plt.ylabel('Arousal')\n",
    "plt.title('DEAP - NNP Label via K-Means')\n",
    "plt.show()\n",
    "print('#Fear: %i - #Sad: %i - #Neutral: %i - Happy: %i'\n",
    "      %(np.count_nonzero(kmeans.labels_ == fear_label),\n",
    "        np.count_nonzero(kmeans.labels_ == sad_label),\n",
    "        np.count_nonzero(kmeans.labels_ == neutral_label),\n",
    "        np.count_nonzero(kmeans.labels_ == happy_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b5ec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_sad = np.where(kmeans.labels_==sad_label)\n",
    "idx_fear = np.where(kmeans.labels_==fear_label)\n",
    "idx_neutral = np.where(kmeans.labels_==neutral_label)\n",
    "idx_happy = np.where(kmeans.labels_==happy_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324e02a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_nnp = np.zeros(Y.shape[0],)\n",
    "Y_nnp[idx_sad] = -1\n",
    "Y_nnp[idx_fear] = -1\n",
    "Y_nnp[idx_neutral] = 0\n",
    "Y_nnp[idx_happy] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff726206",
   "metadata": {},
   "outputs": [],
   "source": [
    "if convert_labels_to_nnp:\n",
    "    Y = Y_nnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37feebbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X: {X.shape}\")\n",
    "print(f\"y: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8fa321",
   "metadata": {},
   "source": [
    "## Safe Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4751752",
   "metadata": {},
   "source": [
    "Please note: This step can take up to several minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1ed2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.savez_compressed(\n",
    "                    output_dir +'/DEAP.npz',\n",
    "                    X=X,\n",
    "                    Y=Y,\n",
    "                    session = session,\n",
    "                    subject = subject,\n",
    "                    trial = trial,\n",
    "                    downsampling_rate = downsampling_rate,\n",
    "                    channel_names = channels_to_use,\n",
    "                    window_size=window_size,\n",
    "                    window_overlap = window_overlap,\n",
    "                    cutoff_frequencies = cutoff_frequencies,\n",
    "                    baseline_removal_window = baseline_removal_window,\n",
    "                    seconds_to_use = seconds_to_use\n",
    "                    )\n",
    "print('Saved File')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045e9ddd",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ed410c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_FFT(X, sampling_rate):\n",
    "    n = len(X) # length of the signal\n",
    "    k = np.arange(n)\n",
    "    T = n/sampling_rate\n",
    "    frq = k/T # two sides frequency range\n",
    "    frq = frq[:len(frq)//2] # one side frequency range\n",
    "\n",
    "    X_FT = np.fft.fft(X)/n # dft and normalization\n",
    "    X_FT = X_FT[:n//2]\n",
    "    return X_FT, frq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725ca1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_id_to_plot = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b68daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ts = X_raw[0,channel_names.index(channels_to_use[channel_id_to_plot]),0:sampling_rate*window_size]\n",
    "preprocessed_ts = X[0,channel_id_to_plot,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf692dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c637e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0,sampling_rate*window_size),raw_ts, label=\"Raw Signal\")\n",
    "plt.plot(np.arange(0,downsampling_rate*window_size)/downsampling_rate*sampling_rate,preprocessed_ts, label=\"Preprocessed Signal\")\n",
    "plt.title(\"Comparison of an Exemplary Time Series\")\n",
    "plt.xticks([0,sampling_rate,2*sampling_rate],[0,1,2])\n",
    "plt.xlabel(\"Time t [s]\")\n",
    "plt.ylabel(\"Voltage U [mV]\")\n",
    "plt.legend()\n",
    "if save_plots_to_file:\n",
    "    plt.savefig(output_dir+'figures/DEAP_Timespace.png', facecolor=\"white\")\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da931e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_fs, raw_frq = perform_FFT(raw_ts, sampling_rate)\n",
    "preprocessed_fs, preprocessed_frq = perform_FFT(preprocessed_ts, downsampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379880e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(raw_frq, abs(raw_fs), label=\"Raw Signal\")\n",
    "plt.plot(preprocessed_frq, abs(preprocessed_fs), label=\"Preprocessed Signal\")\n",
    "plt.title(\"Comparison of an Exemplary Spectrum\")\n",
    "plt.xlabel(\"Frequency f [Hz]\")\n",
    "plt.ylabel(\"Amplitude |X(f)|\")\n",
    "plt.legend()\n",
    "if save_plots_to_file:\n",
    "    plt.savefig(output_dir+'figures/DEAP_Frequencyspace.png', facecolor=\"white\")\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ace6a9",
   "metadata": {},
   "source": [
    "# 数据集含义\n",
    "\n",
    "在机器学习和深度学习中，数据通常被分割成训练集、验证集和测试集。这些数据集分别用于训练模型、调参和验证模型、以及最终评估模型性能。以下是各个数据集的含义：\n",
    "\n",
    "1. 训练集 (Training Set):\n",
    "\n",
    "    - X_train: 训练集的特征数据，输入数据集。\n",
    "    - Y_train: 训练集的标签数据，目标输出。\n",
    "    - S_train: 训练集的受试者ID，表示每个样本对应的受试者。\n",
    "    - P_train: 训练集的试验阶段或试次，表示每个样本对应的实验阶段或试次。\n",
    "\n",
    "2. 验证集 (Validation Set):\n",
    "\n",
    "    - X_val: 验证集的特征数据，用于调参和验证模型。\n",
    "    - Y_val: 验证集的标签数据。\n",
    "    - S_val: 验证集的受试者ID。\n",
    "    - P_val: 验证集的试验阶段或试次。\n",
    "\n",
    "3. 测试集 (Test Set):\n",
    "\n",
    "    - X_test: 测试集的特征数据，用于最终评估模型性能。\n",
    "    - Y_test: 测试集的标签数据。\n",
    "    - S_test: 测试集的受试者ID。\n",
    "    - P_test: 测试集的试验阶段或试次。\n",
    "\n",
    "各个数据集的具体含义如下：\n",
    "\n",
    "- X_train, X_val, X_test: 这些是特征矩阵（例如EEG数据），每一行表示一个样本，每一列表示一个特征。\n",
    "- Y_train, Y_val, Y_test: 这些是标签向量，表示每个样本对应的类别或目标值。\n",
    "- S_train, S_val, S_test: 这些是受试者ID，表示每个样本对应的受试者编号，有助于在跨受试者的实验中进行分析。\n",
    "- P_train, P_val, P_test: 这些是试验阶段或试次，表示每个样本对应的实验阶段或试次编号，有助于在跨试次的实验中进行分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787d79b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 加载DEAP数据集\n",
    "filename = \"E:/Databases/DataPre/DEAP/full_preprocessing/DEAP.npz\"\n",
    "output_dir = \"E:/Databases/DataPre/DEAP/full_preprocessing/\"\n",
    "version = 'bs2'\n",
    "data = np.load(filename)\n",
    "X = data['X']\n",
    "Y = data['Y']\n",
    "subjects = data['subject']\n",
    "sessions = data['session']\n",
    "trials = data['trial']\n",
    "\n",
    "# 分割数据集\n",
    "X_train, X_temp, Y_train, Y_temp, S_train, S_temp, P_train, P_temp = train_test_split(\n",
    "    X, Y, subjects, sessions, test_size=0.4, random_state=42)\n",
    "X_val, X_test, Y_val, Y_test, S_val, S_test, P_val, P_test = train_test_split(\n",
    "    X_temp, Y_temp, S_temp, P_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# 调整subject和session的ID\n",
    "S_train = S_train + 100 * 1\n",
    "S_val = S_val + 100 * 2\n",
    "S_test = S_test + 100 * 3\n",
    "\n",
    "# 保存数据集\n",
    "np.savez(\n",
    "        os.path.join(output_dir, version, 'DEAP.npz'),\n",
    "         X_train=X_train, X_val=X_val, X_test=X_test,\n",
    "         Y_train=Y_train, Y_val=Y_val, Y_test=Y_test,\n",
    "         S_train=S_train, S_val=S_val, S_test=S_test,\n",
    "         P_train=P_train, P_val=P_val, P_test=P_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
